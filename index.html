<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sai Shankar Narasimhan</title>
  
  <meta name="author" content="Sai Shankar Narasimhan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
</head>

<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>
<script>
  function addDarkmodeWidget() {
    new Darkmode().showWidget();
  }
  window.addEventListener('load', addDarkmodeWidget);
</script>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:justify">
                <name>Sai Shankar Narasimhan</name>
              </p>
              <p style="text-align:justify">I am a Research Assistant at <a href="https://robotics.iiit.ac.in/">Robotics Research Centre, IIIT, Hyderabad</a>, working under <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">Madhava Krishna</a>. My interests lie in Robotics and Deep Learning for Computer Vision. Currently I am focussed on Embodied AI tasks related to Object-goal navigation.</p>

              <p style="text-align:justify">I received my undergraduate degree in Electrical Engineering at <a href="https://www.annauniv.edu/">Anna University</a> in India, with my Bachelor's thesis advised by <a href="https://www.linkedin.com/in/ranganath-muthu/">Ranganath Muthu</a>. My Research Internship at <a href="https://www.iitm.ac.in/">IIT Madras</a>, with <a href="https://mech.iitm.ac.in/meiitm/personnal/dr-p-v-manivannan/">P.V.Manivannan</a>, was focussed on SLAM. I have also worked full-time for over a year at <a href="http://www.swaayatt-robots.com/">Swaayatt Robots</a>.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:narasimhansaishankar@gmail.com">Email</a> &nbsp|&nbsp
                <a href="data/resume_1.pdf">CV</a> &nbsp|&nbsp
                <a href="www.linkedin.com/in/sai-shankar-narasimhan">LinkedIn</a> &nbsp|&nbsp
                <a href="https://github.com/saishankarn">Github</a> &nbsp|&nbsp
                <a href="https://twitter.com/nsai_shankar">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:70%;max-width:40%">
              <a href="images/GKV_6227.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/GKV_6227.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li>June 2020: <b>AutoLay</b>, our benchmark for amodal layout estimation was accepted for <b>IROS 2020</b></li>
                <li>Jan 2020: Our work, <b>Deep Flow guided IBVS</b> will be presented in <b>ICRA 2020</b></li>
                <li>Dec 2019: Our work <b>MonoLayout</b> was accepted for <b>WACV 2020</b></li>
                <li>June 2019: Joined <b>Robotics Research Centre</b> at IIIT-Hyderabad as <b>Research Assistant</b></li>
              </ul>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:20%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>


          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <div class="two" id='lh_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/videolayout.gif" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src="images/videolayout.gif" width="300">
              </div>
              <script type="text/javascript">
                function lighthouse_start() {
                  document.getElementById('lh_image').style.opacity = "1";
                }

                function lighthouse_stop() {
                  document.getElementById('lh_image').style.opacity = "0";
                }
                lighthouse_stop()
              </script>
            </td>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>AutoLay: Benchmarking Monocular Layout Estimation</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MnPjDIgAAAAJ&hl=en">Kaustubh Mani*</a>,
              <strong>N. Sai Shankar*</strong>,
              <a href="https://krrish94.github.io/">Krishna Murthy</a>,
              <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">K. Madhava Krishna</a>,
              <br>
        <em>IROS</em> 2020   
              <br>
              Also presented in PAD Workshop, <em>ECCV</em> 2020
              <br>
              <a href="https://sites.google.com/view/pad2020">workshop page</a>
              |
              <a href="https://hbutsuak95.github.io/AutoLay/">project page</a> 
              | 
              <a href="https://ras.papercept.net/proceedings/IROS20/0986.pdf">paper</a>
              <p style="text-align:justify">We introduce AutoLay, a new dataset for amodal layout estimation in birdâ€™s eye view. Further, we propose VideoLayout, a real-time neural net architecture that leverages temporal information from monocular video, to produce more accurate and consistent layouts.</p>
            </td>
          </tr>  



          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <img src="images/monolayout.gif" width="300">
              </div>
            </td>
            <p></p>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>MonoLayout: Amodal Scene Layout from a single image</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=MnPjDIgAAAAJ&hl=en">Kaustubh Mani</a>,
              <a href="https://scholar.google.com/citations?user=F3nRzzMAAAAJ&hl=en">Swapnil Daga</a>,
              <a href="https://deepai.org/profile/shubhika-garg">Shubhika Garg</a>,
              <strong>N. Sai Shankar</strong>,
              <a href="https://krrish94.github.io/">J. Krishna Murthy</a>,
              <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">K. Madhava Krishna</a>,
              <br>
        <em>WACV</em> 2020  
              <br>
              <a href="https://robotics.iiit.ac.in/publications/2020/monolayout-amodal-scene-layout-from-single-image.html">project page</a> 
              | 
              <a href="https://arxiv.org/abs/2002.08394">paper</a>
              |
              <a href="https://github.com/hbutsuak95/monolayout">code</a>
              |
              <a href="https://www.youtube.com/watch?v=HcroGyo6yRQ">video</a>
              <p style="text-align:justify"> We present MonoLayout, a deep neural network for real-time amodal scene layout estimation from a single image. We leverage adversarial feature learning to hallucinate plausible completions for occlusions.</p>
            </td>
          </tr>  


          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <img src="images/dfvs.png" width="300">
              </div>
            </td>
            <p></p>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=ECj2-EIAAAAJ&hl=en">Y V S Harish</a>,
              <a href="https://staff.lincoln.ac.uk/hpandya">Harit Pandya</a>,
              <a href="https://www.linkedin.com/in/ayushgaud/?originalSubdomain=in">Ayush Gaud</a>,
              <a href="https://deepai.org/profile/shreya-terupally">Shreya Terupally</a>,
              <strong>Sai Shankar</strong>,
              <a href="https://www.iiit.ac.in/people/faculty/mkrishna/">K. Madhava Krishna</a>,
              <br>
        <em>ICRA</em> 2020  
              <br>
              <a href="https://robotics.iiit.ac.in/publications/2020/dfvs-deep-flow-guided-scene-agnostic-image-based-visual-servoing.html">project page</a> 
              | 
              <a href="https://arxiv.org/abs/2003.03766">paper</a>
              |
              <a href="https://github.com/harishyvs/FlowBasedIBVS">code</a>
              <p style="text-align:justify"> We propose a novel system consisting of deep neural networks that systematically integrates depth cues with flow features.</p>
            </td>
          </tr>


          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <img src="images/kf.jpg" width="300">
              </div>
            </td>
            <p></p>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>Modified Extended Kalman Filter Using Correlations Between Measurement Parameters</papertitle>
              <br>
              <a href="https://ramanans1.github.io/">Ramanan Sekar</a>,
              <strong>Sai Shankar N</strong>,
              Shiva Shankar B,
              <a href="https://mech.iitm.ac.in/meiitm/personnal/dr-p-v-manivannan/">P.V.Manivannan</a>,
              <br>
        <em>International Conference on Computational Intelligence</em> 2018  
              <br>
              <a href="https://www.iitk.ac.in/idea/ICCI2017/">conference</a> 
              | 
              <a href="https://link.springer.com/chapter/10.1007%2F978-981-13-1132-1_47">paper</a>
              |
              <a href="https://github.com/saishankarn/EKF-Localization">code</a>
              <p style="text-align:justify"> We propose a novel Kalman Filter (KF) algorithm, that leverages the statistical correlation between the measured variables. </p>
            </td>
          </tr>


          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <img src="images/tapenergy.jpg" width="300">
              </div>
            </td>
            <p></p>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>Use of measurement noise correlations for an improved SONAR model</papertitle>
              <br>
              <a href="https://ramanans1.github.io/">Ramanan Sekar</a>,
              <strong>Sai Shankar N</strong>,
              Shiva Shankar B,
              <a href="https://mech.iitm.ac.in/meiitm/personnal/dr-p-v-manivannan/">P.V.Manivannan</a>,
              <br>
        <em>International Conference on Computational Intelligence</em> 2018  
              <br>
              <a href="https://ieeexplore.ieee.org/xpl/conhome/8386712/proceeding">conference</a> 
              | 
              <a href="https://ieeexplore.ieee.org/document/8397253">paper</a>

              <p style="text-align:justify"> We propose a solution to reduce the range and bearing error in SONARs significantly. Using the results from the Gaussian Correlation Inequality, we derive probabilistic transformations that can improve the measurements of the SONAR, thus reducing the sensor error.</p>
            </td>
          </tr>

          <tr onmouseout="lighthouse_stop()" onmouseover="lighthouse_start()">
            <td style="padding:20px;width:40%;vertical-align:justify">
              <div class="one">
                <img src="images/fyp.jpg" width="300">
              </div>
            </td>
            <p></p>
            <td style="padding:20px;width:60%;vertical-align:justify">
                <papertitle>Collaboration between Unmanned Aerial and Ground Vehicles for Search and Rescue Missions</papertitle>
              <br>
              <a href="https://ramanans1.github.io/">Ramanan Sekar</a>,
              <strong>Sai Shankar N</strong>,
              Shiva Shankar B,  
              <br>
              <em>Undergraduate Final Year Project</em>
              <br>
              Supervised by <a href="https://www.linkedin.com/in/ranganath-muthu/">Ranganath Muthu</a>
              <br>
              <a href="data/final_year_project_report.pdf">report</a> 
              | 
              <a href="fyp_presentation.pdf">presentation</a>

              <p style="text-align:justify"> We developed a collaborative Aerial Vehicle (UAV), Ground Vehicle (UGV) platform that can be used to aid / automate search and rescue missions in disaster zones. </p>
            </td>
          </tr>


        </tbody></table>

        <p style="text-align:right">Based on <a href="https://ramanans1.github.io/">Ramanan</a>'s and <a href="https://jonbarron.info/">Jon</a>'s webpages.</p>
        
      </td>
    </tr>
  </table>
</body>

</html>
